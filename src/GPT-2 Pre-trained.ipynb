{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dutch GPT-2-Simple.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"H7LoMj4GA4n_","colab_type":"text"},"source":["#  Train a GPT-2 Text-Generating Model on the Dutch language\n","\n","For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple).\n","\n","\n","To get started:\n","\n","1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n","2. Make sure you're running the notebook in Google Chrome.\n","3. Run the cells below:\n"]},{"cell_type":"code","metadata":{"id":"KBkpRgBCBS2_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"b34ffd62-09d8-4443-d6d1-07db503d3666","executionInfo":{"status":"ok","timestamp":1560858580519,"user_tz":-120,"elapsed":23275,"user":{"displayName":"Zhe Mann","photoUrl":"","userId":"13747572152620027268"}}},"source":["!pip install -q gpt_2_simple\n","import gpt_2_simple as gpt2\n","from datetime import datetime\n","from google.colab import files"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▌                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 22.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 3.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 92kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 286kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 296kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 307kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 317kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 327kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 337kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 348kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 358kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 368kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 378kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 389kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 399kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 409kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 419kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 430kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 440kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 450kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 460kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 471kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 481kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 491kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 501kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 512kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 522kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 532kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 542kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 552kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 563kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 573kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 583kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 593kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 604kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 614kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 624kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 634kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 645kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 655kB 8.0MB/s \n","\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0618 11:49:40.734395 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"v0gyH5K7wWdW","colab_type":"text"},"source":["## Verify GPU\n","\n","Colaboratory now uses an Nvidia T4 GPU, which is slightly faster than the old Nvidia K80 GPU for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text. However sometimes the K80 will still be used.\n","\n","You can verify which GPU is active by running the cell below."]},{"cell_type":"code","metadata":{"id":"r7WvD_8MwYOD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"23f57432-1488-4936-a391-e8fd9138a80e","executionInfo":{"status":"ok","timestamp":1560858581728,"user_tz":-120,"elapsed":24481,"user":{"displayName":"Zhe Mann","photoUrl":"","userId":"13747572152620027268"}}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Tue Jun 18 11:49:41 2019       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   29C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0wXB05bPDYxS","colab_type":"text"},"source":["## Downloading GPT-2\n","\n","If you're retraining a model on new text, you need to download the GPT-2 model first. \n","\n","There are two sizes of GPT-2:\n","    \n","\n","*   `117M` (default): the \"small\" model, 500MB on disk.\n","*   `345M`: the \"medium\" model, 1.5GB on disk.\n","\n","The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory at `/models/117M`.\n","\n","This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."]},{"cell_type":"code","metadata":{"id":"43i81luQ0PSN","colab_type":"code","colab":{}},"source":["model = \"345M\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P8wSlgXoDPCR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"ca854d74-6462-42ea-9e33-5059e15cafc1","executionInfo":{"status":"ok","timestamp":1560858595415,"user_tz":-120,"elapsed":38165,"user":{"displayName":"Zhe Mann","photoUrl":"","userId":"13747572152620027268"}}},"source":["gpt2.download_gpt2(model_name=model)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Fetching checkpoint: 1.05Mit [00:00, 475Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 74.9Mit/s]                                                   \n","Fetching hparams.json: 1.05Mit [00:00, 290Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:12, 113Mit/s]                                  \n","Fetching model.ckpt.index: 1.05Mit [00:00, 215Mit/s]                                                \n","Fetching model.ckpt.meta: 1.05Mit [00:00, 49.4Mit/s]                                                \n","Fetching vocab.bpe: 1.05Mit [00:00, 92.8Mit/s]                                                      \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"N8KXuKWzQSsN","colab_type":"text"},"source":["## Mounting Google Drive\n","\n","The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n","\n","Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"]},{"cell_type":"code","metadata":{"id":"puq4iC6vUAHc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"c678c3f6-6fb5-42ff-a898-52574f2f1c02","executionInfo":{"status":"ok","timestamp":1560858630122,"user_tz":-120,"elapsed":72872,"user":{"displayName":"Zhe Mann","photoUrl":"","userId":"13747572152620027268"}}},"source":["gpt2.mount_gdrive()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"22mPJZ7ASuXI","colab_type":"text"},"source":["## Select model to train\n"]},{"cell_type":"code","metadata":{"id":"J7aZpoYcqYBL","colab_type":"code","colab":{}},"source":["columns_only_15k = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6OFnPCLADfll","colab_type":"code","colab":{}},"source":["# Init run_name and dir_name to load datasets\n","if columns_only_15k:\n","  run_name = '345M_columns_only_15k'   \n","else:\n","  run_name = '345M_columns_only_30k'   \n","run_name_checkpoint = 'checkpoint/' + run_name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"phUsNUNMIaMI","colab_type":"code","colab":{}},"source":["# Init run_name and dir_name to load datasets\n","run_name = '345M_all_datasets'   \n","run_name_checkpoint = 'checkpoint/' + run_name"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NvDTBAnP4jIU","colab_type":"text"},"source":["## Copy encoded dataset"]},{"cell_type":"markdown","metadata":{"id":"ixFXmYKP2gZL","colab_type":"text"},"source":["Select two cells below for copying all encoded datasets (columns, books, wiki-pages)"]},{"cell_type":"code","metadata":{"id":"ANRKmCVa4TEF","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/data/encoded_books_columns_wiki/ /content/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S1Vq5x32EBx","colab_type":"code","colab":{}},"source":["dir_name = \"encoded_books_columns_wiki\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s1SdCWiX2onN","colab_type":"text"},"source":["Select two cells below for copying only the encoded columns and books datasets"]},{"cell_type":"code","metadata":{"id":"cod0Ewe-43mn","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/data/encoded_books_columns/ /content/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVdu_9w30GgO","colab_type":"code","colab":{}},"source":["dir_name = \"encoded_books_columns\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXRq4AyE2vYU","colab_type":"text"},"source":["Select two cells below for copying only the encoded columns dataset"]},{"cell_type":"code","metadata":{"id":"reYlXwd-20r4","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/data/encoded_columns/ /content/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-z8lH5YR225P","colab_type":"code","colab":{}},"source":["dir_name = \"encoded_columns\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BT__brhBCvJu","colab_type":"text"},"source":["## Uploading a Text File to be Trained to Colaboratory\n","\n","Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."]},{"cell_type":"markdown","metadata":{"id":"HeeSKtNWUedE","colab_type":"text"},"source":["If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."]},{"cell_type":"code","metadata":{"id":"tUdqsJKlZQP5","colab_type":"code","colab":{}},"source":["mkdir data; cd data;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z6okFD8VKtS","colab_type":"code","colab":{}},"source":["gpt2.copy_file_from_gdrive(file_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePq_9AeJzh3q","colab_type":"text"},"source":["If dataset is > 100MB it is advised to encode it as this improves performance GPU-wise."]},{"cell_type":"code","metadata":{"id":"ibPixTcAzS3T","colab_type":"code","colab":{}},"source":["gpt2.encode_dataset(file_name, model_name=\"345M\", out_path=\"columns_encoded.npz\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ET6cGZL2QUi","colab_type":"code","colab":{}},"source":["!cp columns_encoded.npz /content/drive/My\\ Drive/data/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pel-uBULXO2L","colab_type":"text"},"source":["## Load a Trained Model Checkpoint\n","\n","Running the next cell will copy the `checkpoint` folder from your Google Drive into the Colaboratory VM."]},{"cell_type":"code","metadata":{"id":"DCcx5u7sbPTD","colab_type":"code","colab":{}},"source":["gpt2.copy_checkpoint_from_gdrive(run_name=run_name, copy_folder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTa6zf3e_9gV","colab_type":"text"},"source":["The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n","\n","**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."]},{"cell_type":"code","metadata":{"id":"-fxL77nvAMAX","colab_type":"code","colab":{}},"source":["sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, run_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdpZQXknFNY3","colab_type":"text"},"source":["## Finetune GPT-2\n","\n","The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n","\n","The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n","\n","The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n","\n","**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n","\n","Other optional-but-helpful parameters for `gpt2.finetune`:\n","\n","\n","*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n","* **`sample_every`**: Number of steps to print example output\n","* **`print_every`**: Number of steps to print training progress.\n","* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n","*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)"]},{"cell_type":"code","metadata":{"id":"aeXshJM-Cuaf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1380},"outputId":"3bcfc4c1-3348-46c7-f5ac-0e13146a2e96"},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.finetune(sess,\n","              dataset=dir_name,\n","              model_name=model,\n","              steps=15000,\n","              restore_from='latest',\n","              print_every=50,\n","              sample_every=100,\n","              save_every=5000,\n","              learning_rate=0.0001,\n","              run_name=run_name\n","              )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0618 11:51:55.278429 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:90: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0618 11:51:55.280948 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:100: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","W0618 11:51:56.553875 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:164: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0618 11:51:56.562221 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","W0618 11:52:10.690787 140547183056768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:71: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0618 11:52:10.713631 140547183056768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0618 11:52:10.720380 140547183056768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.random.categorical` instead.\n","W0618 11:52:10.737257 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:191: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","W0618 11:52:30.300697 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:198: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","W0618 11:52:30.304136 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:200: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","W0618 11:52:30.311457 140547183056768 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:202: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","W0618 11:52:43.464157 140547183056768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading checkpoint checkpoint/345M_columns_only_30k/model-30000\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 22.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loading dataset...\n","dataset has 789166 tokens\n","Training...\n","Saving checkpoint/345M_columns_only_30k/model-30000\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["======== SAMPLE 1 ========\n","ig een uitroepteken waar de drie vlot van de Tweede Wereldoorlog staat. Zodat volgens de verhalen die je vermoedt, pakt je schrift (ik vat de titel), je ging je nieuws en je freak daar een eet. Ooit heb ik gelezen dat het kan behoorlijk best aan Nederland.’’\n","<|endoftext|>\n","Zeventig jaar geleden was de oorlog afgelopen. Uitgeverij Van der Meulen stond op het idee van de film ‘Catch 22’, in een poging het antwoord er als motifs voor dag en lacht. Vooral de vijf voor het zidgeel rolde het in Nederlandse film ‘Zero point’, in een Ria Valkhuis.\n","Nou, als ik het logisch was, zou het iets niet lekker zijn.\n","Morgen worden ze enkel een miljoen Nederlanders die er het uit willen. De vraag is natuurlijk: is het voor het programma, ben je een lijstklap? Dat wil de Laura Nederlanders van Kimono kunnen belegen, vorige week was er een 000-rood boerderij om iets te ontwerpen.\n","Nou, dan zou ik wel het haar las. Iking.\n","Er zijn nog twee Nederlanders die erppen, en meestal deel ze nog, om te melden dat het per persoon per jaar voor iets is geworden. Gooi je ioer pakken: hoe kess werkt de streek?\n","Maar wacht maar, de Stelr trio, die altijd weer meer dan vijftig jaar geleden zijn als ze de caissière dovet zijn met een goed functionerende streek. Als je ze netjes naar Stelr bedrét, kun je er dood aan.\n","Stelr is gewoon een hotelkamer. Het is misschien dood, rabje knalde, wat vindt de voorraad er immeer dan vijftig jaar geleden? Caas van de roman waar iedereen zo voorlopen, eveel werkt! Zijn ze op Ameland met zijn allen trots op te richten, maar het was in de andere greep af.\n","Nee, dat is misschien wel ... een hotelkamer met wieltjes. Als we aan de hand waren, los van een opvallende brug, bep ver weg en toe: ‘Weet u wat er gebeurd is met de auto’. Gooi je jezelf wat dat is.\n","Liefst In Onze Dom van Koningin (11) die ik nooit had gekregen, schrijft altijd een doodsbedrijf over een konijnenventiel.\n",",,Ik had het er met de aarts van Koningin van der Meulen”, vertelt hij als oude man. ,,Het was een aarts van die deze bezetting. Maar die kolereertijd had ik er weer in.” Ik ben er net begonnen.\n","<|endoftext|>\n","Er zijn zoveel citaten over uitstel, dat dit wel een van de grote problemen van de mensheid moet zijn. We willen van alles, de tijd is beperkt, toch komt het er vaak niet van.\n",",,Sloege soe ek, mar hy stoar earder”, zei mijn pake. Volgens Picasso moet je alleen die dingen tot morgen uitstellen, waarvan het niet erg is als ze nooit gebeuren. Lao Tse orakelde: een reis van 1000 mijl begint met de eerste stap. Dat was in de zesde eeuw voor de jaar al geschreven, maar\n","\n","[30050 | 210.99] loss=0.08 avg=0.08\n","[30100 | 357.68] loss=0.11 avg=0.10\n","======== SAMPLE 1 ========\n"," wat dat staat oboelectooi in de tot klap is. Tot nu toe boeide het.\n","Deze week vertrokken we. Vogelijke Nederlanders hebben doodstrekkers en bevinderen.\n","Piet Kloppers en laterrijsels bevestigden me. Ik had frisdrank kunnen nemen, maar ik heb niet naar Zeeland gepraat. De enige die over een paar keer in Nederland zat is Zijlstra’s Misschien.\n",",,Ik denk dat het komt”, zei ik. ,,Maar ik weet niet wie een piemel over sjongende wy kon maken.”\n",",,Een keer meer”, zei de man. ,,Die is even. De ligt nog even.”\n","Het gaat om een totging.\n","Een nieuwe totging, nummer 27. ,,De ogenvallende koppie in de Slag was systeem.”\n",",,Je hebt natuurlijk de dijk”, zei de vrouw. ,,Ninety-techniek rotzooi.”\n","‘Ik bekque de hoogartstewartigst’ stak ik mijn medische opdracht.\n",",,Ik ben ook een hogere cent doen’’, zei de man. ,,Het is maar vlot ook raar.”\n","Het was een eenvoudig gezin, eerst woonden ze op een ruit of een donker jasje.\n",",,Ik had hem nu een kaartje.”\n","Zijn vrouw krijgt boven een vinger op, maar dat wijste hier geen cent. De man krijgt ze altijd enthousiast.\n","Door ze broodje hebben gezegd dat het een normale kleinkinderen is van een normale kleinkinderen.\n","Zo krijgt Leeuwarden 2024, waar hij kwonen met een echte Berliner emmers over.\n","Daar is het rondje, nu. De kleinkinderen zijn beter dan het\n","Vienna, maar de usst van het Nederlandse capitali voorlopig is er het niet.\n","Winters heeft een onuitputtelijk bevestigd. Bij de dingen van de rennen staat dat geen rennen om te weten, maar de kleur beheerste is dat de rennen zijn bedriegt. Bij de plerse halen bij zijn kopie zijn kans op met bouwfraude (,,booofgooode’’) en strafbaar seminarieslaapzaaligheid.\n","Lifelines insturen zijn gewone rennen waar de deur op gleed staat, schijnt te zijn te hebben dat het zelfs onder de armen verdiende elke keer is geweest. De man op de bank met het trophy voor de kunst en de systeem uit de recensie.\n",",,De meeste bezoekers zijn ook wel hun jongens begin jaren tachtig wonen, maar die scheiden zo’n grote zaken zouden verzuipen. Als jongens elder verdienen, verschillende, ook al leken de deur op. Die kwam ook naar de auto in de jeugd, tussen de drivekundigde als zo’n dingetje. Het zag er nogal wat. De kiezels en stenen die in de tuin zitten toen persoonlijk waren, waren uit de voetgaten.\n","Elke keer was het onnozellijk wie. De opening was pas nog in het Jiddisch te kletsen, hij reed zijn opening van een jatje op met een kopje of iets afschuwelijks in de grond. Die jongenhospitaal was meer dan een moord en vooral op\n","\n","[30150 | 541.06] loss=0.11 avg=0.10\n","interrupted\n","Saving checkpoint/345M_columns_only_30k/model-30170\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IXSuTNERaw6K","colab_type":"text"},"source":["After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n","\n","If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive."]},{"cell_type":"code","metadata":{"id":"VHdTL8NDbAh3","colab_type":"code","colab":{}},"source":["gpt2.copy_checkpoint_to_gdrive(run_name=run_name, copy_folder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQJgV_b4bmzd","colab_type":"text"},"source":["You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."]},{"cell_type":"markdown","metadata":{"id":"ClJwpF_ACONp","colab_type":"text"},"source":["## Generate Text From The Trained Model\n","\n","After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."]},{"cell_type":"code","metadata":{"id":"4RNY6RBI9LmL","colab_type":"code","colab":{}},"source":["gpt2.generate(sess, return_as_list=True, run_name=run_name[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSLFLCY_FoG-","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"oF4-PqF0Fl7R","colab_type":"text"},"source":["If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n","\n","You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n","\n","You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n","\n","Other optional-but-helpful parameters for `gpt2.generate` and friends:\n","\n","*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n","* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n","* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n","*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."]},{"cell_type":"code","metadata":{"id":"8DKMc0fiej4N","colab_type":"code","outputId":"61579f4b-611b-4f13-b188-18bc50a8aeb5","executionInfo":{"status":"ok","timestamp":1560788465295,"user_tz":-120,"elapsed":48250,"user":{"displayName":"Zhe Mann","photoUrl":"","userId":"13747572152620027268"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["gpt2.generate(sess,\n","              length=300,\n","              temperature=0.7,\n","              prefix=\"Als er een eretitel Drukste Toeristenstadje Ter Wereld bestond, was Mérida, de stad in de Andes waar ik een paar jaar geleden was, een kanshebber.\",\n","              include_prefix=True,\n","              truncate=\"<|endoftext|>\",\n","              nsamples=5,\n","              batch_size=1,\n","              run_name=run_name,\n","              top_k = 40\n","              )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Als er een eretitel Drukste Toeristenstadje Ter Wereld bestond, was Mérida, de stad in de Andes waar ik een paar jaar geleden was, een kanshebber. Een retitel die rhytterde als een boom. ,,Geen mensen die er rhyt tegenaan’’, zei ik. ,,De vader zegt vooral: ‘Wij wensen uit ons Beursgenot’.’’\n","Zelf was ik niet rhyt, het waren wel voorouders die ik derde klasgenoot in de lagere school heb gehaald. Rapp stond erom bekend dat je uit dat verhaal niet eens een echte boom mag heten.\n","Maar de bame was niet misschien helemaal rhyt, het is geen mooie gedachte. En als die mop nog heet, zegt u dat ook allemaal die twee oude ooms, die elkaar in de gang hadden gezeten. Die waren akademien in 1974 ook verderop gaan.\n","De vader van de boom was daar al mee op de klas geweest, hij was de eerste die er in rij mensen die ik ernaar vroeg. ,,Dat heb ik niet gedaan’’, zei hij\n","====================\n","Als er een eretitel Drukste Toeristenstadje Ter Wereld bestond, was Mérida, de stad in de Andes waar ik een paar jaar geleden was, een kanshebber.\n","Ik stond in de voorstelling met de Toeristenstate of de Toeristenhoek, maar ik had een beslissing tegenover de Voorstreek. Als ik in zo’n flagge pand was, zat ik ook aan tafel. Want de Voorstreek is best een toeristengebied. Het is een sfeer vol linten en grote hoeven, de bovenbouw moet voor de hemel zijn, want je kunt er een paleis bouwen.\n","Dus hoef ik niet eens te zien op een cabriolet, want dit toeristengebied is dat van nature werk. Wennet daar mee weg.\n","\n","====================\n","Als er een eretitel Drukste Toeristenstadje Ter Wereld bestond, was Mérida, de stad in de Andes waar ik een paar jaar geleden was, een kanshebber.\n","Ik geloof dat Mérida de mooiste stad van Noord-Amerika is. Het grootste aanbod van misdaadticketen is er bij Sneek, bijvoorbeeld, waar een vrouw de hele avond met de beide jerieuwslettertype van de straat aan opstapt.\n","Ja, ik zit een kans schoon. Om naar u te kijken, ik ben een dagje ouder. De rest is er een slag geweest, het zat er moet he.\n","Dat is het namelijk. Mijn dagelijkse leven voor de jeugd was dus al klaar. Ik heb er op het ogenblik een vaste baan kunnen tappen.\n","Maar dat kwam me pas aan toen ik vrijdag een aspirant-progressionele danszaal bekeek in het pandacentrum van het framencraft helakiotisch zoopmuurtje van het Virginie.\n","Dit was me te gierig om te zien. Ik had er een hele opgave van. Destijds was ik nog nota bene in mijn boekje vol\n","====================\n","Als er een eretitel Drukste Toeristenstadje Ter Wereld bestond, was Mérida, de stad in de Andes waar ik een paar jaar geleden was, een kanshebber.\n","Ik stond in de rij bij de bocht, de bocht in de zaak, de gids, de bedieningkast, de fietsenstalling, de gepensioneerden lagen aan het plaatsje.\n",",,Geeft u bovenop aan desi?”, vroeg een dame, die in de voorstelling een boek had gemaakt. ,,Hebt u bovenop aan desi?”, zei ik. ,,Nee”, zei ze. Mijn verblijf klopte, ze had het achter de rug en ik begon er op te wennen.\n","Mijn interesse was gewekt. Ik besloot na zijn verhaal over Lula zomaar ineens te gaan klappen. Lula was de halfronde van het park. Ik had haar verzonnen, misschien had ze op de achterbank geboekt, maar ik wist niet, zij was naast me. De mevrouw van het park had een kast vol leesbedragen, vertelde ze. ,,Was jouw opa ook zo clawig?”,\n","====================\n","Als er een eretitel Drukste Toeristenstadje Ter Wereld bestond, was Mérida, de stad in de Andes waar ik een paar jaar geleden was, een kanshebber.\n","Ik stel voor dat ik hier mijn eerste misdaad zal zijn.\n","Maar toch: als ik hier zit, dan is het een heel gewoon boekot.  \n","\n","====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l0GYUlBSydAQ","colab_type":"code","outputId":"8ec3bb7c-b308-485c-a62f-ef4b5e5c4e52","executionInfo":{"status":"ok","timestamp":1560792396119,"user_tz":-120,"elapsed":49065,"user":{"displayName":"Zhe Mann","photoUrl":"","userId":"13747572152620027268"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["gpt2.generate(sess,\n","              length=300,\n","              temperature=0.7,\n","              prefix=\"We remden bij een dorpje, omdat een veldje met bomen vol opgewonden mannen stond.\",\n","              include_prefix=True,\n","              truncate=\"<|endoftext|>\",\n","              nsamples=5,\n","              batch_size=1,\n","              run_name=run_name,\n","              top_k = 40\n","              )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["We remden bij een dorpje, omdat een veldje met bomen vol opgewonden mannen stond. We remden bij nacht op straat, omdat er een opgewonden vrouw in de zaal zat. En omdat er op het dak bijna geen vrouw in de zaal was. We remden bij nacht op straat omdat er op gebeurtenissen gebeurtenissen zijn met namen als Khalil, Mohamad, Adelanto, Ady Sidonia, Afrika en Nabobje.\n",",,U zat toch bij de carnavalsspelen?”, vroeg ik mijn overbuurvrouw M. in de tent. ,,Wordt u al geholpen?” ,,Ja, u ging als een heilige naar de nieuwe bioscoop, waar hij boeken uit het hart van Godleken kon zien. Dat klonk als lytse Hille.”\n","M. en zijn vrouw waren erbij, die kwamen uit Heerenveen. Ze hadden een caravan in Bant, bij de uitlaat van de Prinsentuin, waar naar verwijzingen IJlst en Dalen even bij kon.\n","Zelf had\n","====================\n","We remden bij een dorpje, omdat een veldje met bomen vol opgewonden mannen stond. We remden bij Dútsjes, omdat we aan het banaan hadden te klagen. We remden bij de provincie, omdat we daar spenden en drukken. We remden bij de plaats en de hire car in de plaats van de camper, omdat rede dat je naar een plaats die anders verder kunt onthouden. We remden bij de plaats en het gebied, omdat we daar heen remmen. We remden bij het gasthuis, omdat het overstrooide land er inhield. We remden bij het ongeluk, omdat het overstrooide land er inhield. We remden bij het gasthuis, omdat het overstrooide land er inhield. We remden bij het ongeluk, omdat het overstrooide land er had ingeblikt. We remden bij het ongeluk, omdat het al met al ergreifelijk was. We remden bij de plaats en de hire car in de plaats, omdat het overstrooide land er had ingeblikt. We remden bij he\n","====================\n","We remden bij een dorpje, omdat een veldje met bomen vol opgewonden mannen stond. We remden bij D-Day, de persconferentie voor de eerste vlucht naar Engeland in 1944. We remden bij de aanslag op Leeuwarden in 1945.\n","Terwijl ik dit schrijf beneden aan de praat van Death Wish, de dominee die je op mijn schoot had tikt gestoken als een kat.\n","Zwijgend holde ik de demping aan. Het was zo duidelijk als een steen. Zelfs dat kon ik meteen onthouden dat ik Death Wish zou leren.\n","Later kwam Death Wish tegen en verhalen over dat er in het dierenasiel de klok lagen. Het was of er een tegelvloer was aan de binnenkant, maar dat was er niet geweest.\n","Op een nacht stond Death Wish in de menagerie. Ik dacht dat het deiesde, maar daar ging het om. Het was of er een tegelvloer was aan de binnenkant.\n","Daar draaide ik Death Wish zelf. Ebene die van de aanslag kwam om het le\n","====================\n","We remden bij een dorpje, omdat een veldje met bomen vol opgewonden mannen stond. We remden bij Leeuwarden, omdat we daar met de bomen stonden.\n","Het was van die bomen, die als een tether aan de overkant van de straat stond. Mensen tussen de riem drongen met handen en programmaboekjes in de mond. Ze mompelden en klapten, terwijl de bomen stil sidderen.\n","Toen de stoet gepasseerd was, hoestte ik nog steeds. Misschien heeft shet verkeer van Dallas bovenop de stoep geen rol gespeeld, maar dat was het prachtige van de film. Het zijn mensen die misleid door de bomen heen loopt, maar die denken daar toch al snel aan dat er iets mis is geweest.\n",",,Ik denk dat de bomen stil zijn’’, zei een jongen. Hij had er meer met de bomen van de Voorstreek en Colegemoord en ik wachtte een moment er waarop hij weer eens goinged naar huis zou gaan.\n",",,De bomen stil zijn, die zijn er\n","====================\n","We remden bij een dorpje, omdat een veldje met bomen vol opgewonden mannen stond. We remden bij nacht, want de bomen stonden stil in de diepte. Midden op de vloer tikten ze een hertenfiguurtje op.\n",",,We hebben hem zelf uitgezocht’’, zei een van ons, die in de buurt woonde. ,,Hij is immers toch zelf uitgezocht.’’\n","Dat was een tegenvaller na een dag noeste bomen. Dan schept het bloem ineens. Dat is mijn grote liefde nu, mijn laatste liefje.\n","\n","====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b0EYQsZhygkm","colab_type":"code","colab":{}},"source":["gpt2.generate(sess,\n","              length=300,\n","              temperature=0.7,\n","              prefix=\"Woensdagavond mocht ik met de watertaxi naar de vaste wal. Die is er al bijna 25 jaar maar ik was er nog nooit mee meegevaren.\",\n","              include_prefix=True,\n","              truncate=\"<|endoftext|>\",\n","              nsamples=5,\n","              batch_size=1,\n","              run_name=run_name,\n","              top_k = 40\n","              )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3Mk9L7uyhIR","colab_type":"code","colab":{}},"source":["gpt2.generate(sess,\n","              length=300,\n","              temperature=0.7,\n","              prefix=\"Een rare tijd, net voor kerst, vooral als je helemaal niet in kerststemming bent.\",\n","              include_prefix=True,\n","              truncate=\"<|endoftext|>\",\n","              nsamples=5,\n","              batch_size=1,\n","              run_name=run_name,\n","              top_k = 40\n","              )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjjEN2Tafhl2","colab_type":"text"},"source":["For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp and then download it.\n","\n","You can rerun the cell as many times as you want for even more generated texts!\n","\n","Run one of the cells below regarding whether you have pre- and suffixes."]},{"cell_type":"code","metadata":{"id":"e8ZgcDeHs1tW","colab_type":"code","colab":{}},"source":["gen_file = 'gpt2_simple_{:%Y%m%d_%H%M%S}_iter=10k_t=0.7.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=320,\n","                      temperature=0.7,\n","                      truncate=\"<|endoftext|>\",\n","                      nsamples=50,\n","                      batch_size=10,\n","                      run_name=run_name\n","                      )\n","\n","files.download(gen_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zREFkVWJ-jz-","colab_type":"code","colab":{}},"source":["gen_file = 'gpt2_simple_{:%Y%m%d_%H%M%S}_iter=10k_t=0.9.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=320,\n","                      temperature=0.9,\n","                      truncate=\"<|endoftext|>\",\n","                      nsamples=50,\n","                      batch_size=10,\n","                      run_name=run_name\n","                      )\n","\n","files.download(gen_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkaNjefvv9dI","colab_type":"code","colab":{}},"source":["gen_file = 'gpt2_simple_{:%Y%m%d_%H%M%S}_iter=10k_t=1.1.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=320,\n","                      temperature=1.1,\n","                      truncate=\"<|endoftext|>\",\n","                      nsamples=50,\n","                      batch_size=10,\n","                      run_name=run_name\n","                      )\n","\n","files.download(gen_file)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ig-KVgkCDCKD","colab_type":"text"},"source":["# Etcetera\n","\n","If the notebook has errors (e.g. GPU Sync Fail or out-of-memory/OOM), force-kill the Colaboratory virtual machine and restart it with the command below:"]},{"cell_type":"code","metadata":{"id":"rIHiVP53FnsX","colab_type":"code","colab":{}},"source":["!kill -9 -1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmTXWNUygS5E","colab_type":"text"},"source":["# LICENSE\n","\n","MIT License\n","\n","Copyright (c) 2019 Max Woolf\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE."]}]}