{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT-2 117M From scratch [Columns Books Wiki].ipynb","version":"0.3.2","provenance":[{"file_id":"1XgBcNHYVzAco7u8gob1ufkwuw9Ey-BWa","timestamp":1559284688322},{"file_id":"11PD9KRkOYggnz9-rq32buldSCXQKgxgR","timestamp":1557833172045},{"file_id":"1D5t3w5mnCuEkGp3kBxYOHLVCzLqoXYgJ","timestamp":1556095234725}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oeJ6PyPjb_o5","colab_type":"text"},"source":["#  1. Model 3 - GPT-2 117M  From scratch model \n","\n","1) Make sure to enable GPU -> Edit > Notebook Settings > Hardware accelarator\n","\n","To start:\n","* Execute all cells belonging to step 1\n","\n","Then, to generate sample texts:\n","* Execute cells belonging to step 2\n","\n","Or, to fine-tune the model:\n","* Execute cells belonging to step 3"]},{"cell_type":"markdown","metadata":{"id":"qjBqj8m6za78","colab_type":"text"},"source":["**Note:** Colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."]},{"cell_type":"markdown","metadata":{"id":"ib-BLMie0G1N","colab_type":"text"},"source":["### 1.1 Mount Google Drive\n","Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account"]},{"cell_type":"code","metadata":{"id":"m_1nmGKy0HW_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4k6sH9KfRFDi","colab_type":"text"},"source":["## Step 1.2 Verify GPU\n","\n","Colaboratory now uses an Nvidia T4 GPU, which is slightly faster than the old Nvidia K80 GPU for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text. However sometimes the K80 will still be used.\n","\n","You can verify which GPU is active by running the cell below."]},{"cell_type":"code","metadata":{"id":"S-bJScukcdak","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDosNBPIz5aK","colab_type":"text"},"source":["## Step 1.3 Clone custom repo\n","Clone the custom made git repository to fine-tune GPT-2 from scratch. Install the requirements in addition."]},{"cell_type":"code","metadata":{"id":"WFLQv2e0_Vfh","colab_type":"code","colab":{}},"source":["!git clone https://github.com/zhemann/gpt-2.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxCbG9Bjz8bt","colab_type":"code","colab":{}},"source":["cd gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-DgbAqiz-my","colab_type":"code","colab":{}},"source":["!pip3 install -r requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkkgR1kFVDLy","colab_type":"text"},"source":["## Step 1.4 Download, build and install Sentencepiece\n","See also [sentencepiece](https://github.com/google/sentencepiece)\n"]},{"cell_type":"code","metadata":{"id":"lfosO3AGH8YF","colab_type":"code","colab":{}},"source":["!pip3 install sentencepiece"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7gQmmcnYZKm","colab_type":"code","colab":{}},"source":["cd /content"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7iJYbaFkr-Q","colab_type":"code","colab":{}},"source":["%%bash -e\n","if ! [[ -f ./spm_train ]]; then\n","  wget https://github.com/google/sentencepiece/archive/v0.1.82.zip\n","  unzip v0.1.82.zip\n","fi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAabbnBUkx--","colab_type":"code","colab":{}},"source":["% cd sentencepiece-0.1.82\n","% mkdir build\n","% cd build"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOuzTlSDYqkQ","colab_type":"code","colab":{}},"source":["!cmake .."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3k95fM-Yus3","colab_type":"code","colab":{}},"source":["!make -j $(nproc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvZxA5jZYwpb","colab_type":"code","colab":{}},"source":["!sudo make install"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUI4bRc6YywL","colab_type":"code","colab":{}},"source":["!sudo ldconfig -v"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iDJ5QMvKjda","colab_type":"text"},"source":["## Step 1.5 Set Python IO Encoding"]},{"cell_type":"code","metadata":{"id":"mRj2wvH80O76","colab_type":"code","colab":{}},"source":["!export PYTHONIOENCODING=UTF-8"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nn9OsyyX_IHf","colab_type":"text"},"source":["## Step 1.6 Load trained model\n","Load the trained model. Creates directory 'models' first."]},{"cell_type":"code","metadata":{"id":"Qk0uJNN0Ak9y","colab_type":"code","colab":{}},"source":["cd /content/gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2qoJy2x1c82","colab_type":"code","colab":{}},"source":["mkdir models"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"brjK4CRf_nF_","colab_type":"text"},"source":["Load model from drive into 'models' directory"]},{"cell_type":"code","metadata":{"id":"0l8vFo2s-K9k","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/checkpoint_from_scratch/117MSP /content/gpt-2/models/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v84AMZ1MSW5C","colab_type":"text"},"source":["# 2. Generating sample texts\n","To generate samples, please stick to this section.\n","\n","See section **3. Fine-tuning the model** if you want to fine-tune the model."]},{"cell_type":"markdown","metadata":{"id":"-2S093Wm0sNZ","colab_type":"text"},"source":["## Step 2.1 Generate samples\n","Generate conditional samples from the model given a prompt you provide - change top-k hyperparameter if desired (default is 40).  "]},{"cell_type":"code","metadata":{"id":"Zv-QLavtbVqa","colab_type":"code","colab":{}},"source":["cd /content/gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJt6SwKJx3Jy","colab_type":"code","colab":{}},"source":["!python3 src/interactive_conditional_samples.py --top_k 40 --temperature 0.5 --length 300 --model_name '117MSP' --nsamples 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5ZnPEjo0uQl","colab_type":"text"},"source":["To check flag descriptions, use:"]},{"cell_type":"code","metadata":{"id":"nxrTJZXh0zRo","colab_type":"code","colab":{}},"source":["!python3 src/interactive_conditional_samples.py -- --help"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"driWn2UcTCJf","colab_type":"text"},"source":["# 3. Fine-tuning the model\n","This section can be used to fine-tune the current model. Since the '117MSP'  directory already contains the encoded datasets, we do not need a separate command to copy the encoded datasets from Google Drive to the Colaboratory VM, but can start training the model right away.\n"]},{"cell_type":"markdown","metadata":{"id":"1tzRSYqU0aJG","colab_type":"text"},"source":["## Step 3.1 Train model\n","Start training and save model to Google Drive afterwards."]},{"cell_type":"code","metadata":{"id":"38YrSc21_tsW","colab_type":"code","colab":{}},"source":["cd /content/gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OX57AA2k0hR8","colab_type":"code","colab":{}},"source":["!PYTHONPATH=src ./train.py --dataset models/117MSP/dataset_columns_enc.npz --model_name '117MSP' --steps 5000 --sample_every 1000 --save_every 4000 --learning_rate 2.5e-4 --run_name run1 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-pVMW2t0mlX","colab_type":"code","colab":{}},"source":["!cp -r /content/gpt-2/models/117MSP/ /content/drive/My\\ Drive/checkpoint_from_scratch/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DRmGAtPOTHjZ","colab_type":"text"},"source":["# 4 Training SentencePiece and encoding the dataset\n","This section is dedicated to show you how the trained SentencePiece model is trained (the vocabulary files) as well as how the raw datasets are concatenated and encoded. \n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"4-GErF9vwqYV","colab_type":"text"},"source":["## Step 4.1 Copy dataset\n","1.   Create directory '`data`' within '`gpt-2`' directory\n","2.   Copy either one, two or three datasets from Google Drive into the '`data`' directory by running the corresponding cells\n","\n"]},{"cell_type":"code","metadata":{"id":"nLUVPAx-WK7I","colab_type":"code","colab":{}},"source":["cd /content/gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDnuMDSOm15y","colab_type":"code","colab":{}},"source":["mkdir data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_6AQC6qWQId","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/data_from_scratch/wiki_raw.txt /content/gpt-2/data/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4ThdS3gwQLz","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/data_from_scratch/columns_raw.txt /content/gpt-2/data/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7taZhE0fHE0E","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/My\\ Drive/data_from_scratch/books_raw.txt /content/gpt-2/data/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzFpXWMMnc1t","colab_type":"text"},"source":["## Step 4.2 Create dictionary files\n","Within these step, the following actions are performed:\n","1. Combine all .txt-files in directory gpt-2/data into one large .txt-file.\n","2. Create dictionary files based on large .txt-file"]},{"cell_type":"code","metadata":{"id":"29zwiwp4VltB","colab_type":"code","colab":{}},"source":["cd /content/gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RjvyvMeT-Ph","colab_type":"text"},"source":["First, we run concat.sh to create one dataset from multiple files and to add custom newline tokens <|n|> to the datasets. This is necessary as SentencePiece does not add such a token to the dictionairy automatically."]},{"cell_type":"code","metadata":{"id":"F6Ahi3NVNft3","colab_type":"code","colab":{}},"source":["!sh scripts/concat.sh data datasets_combined.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8Tst0CpUIIM","colab_type":"text"},"source":["Then, we copy the generated text-file to the '`data`' directory"]},{"cell_type":"code","metadata":{"id":"mYOOYdNQum2D","colab_type":"code","colab":{}},"source":["!cp datasets_combined.txt data/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MT_wbdfaUOW0","colab_type":"text"},"source":["The next cell performs the actual training of the SentencePiece model takes place. This process will create the following three files:\n","\n","\n","1. hparams.json\n","2. sp.model\n","3. sp.vocab\n","\n"]},{"cell_type":"code","metadata":{"id":"fzb7vya9W87M","colab_type":"code","colab":{}},"source":["!sh scripts/createspmodel.sh data/datasets_combined.txt 40000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"owa5GTwNnye0","colab_type":"code","colab":{}},"source":["mkdir models; cd models; mkdir 117MSP_Test;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNxfdksexVZB","colab_type":"code","colab":{}},"source":["cd /content/gpt-2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7jsUZCci6Z1","colab_type":"text"},"source":["We created a new directory called ''`117MSP_Test`' in the previous step, and in addition we add the three files to this directory."]},{"cell_type":"code","metadata":{"id":"CJC4SXVuBcCb","colab_type":"code","colab":{}},"source":["!cp hparams_117M.json models/117MSP_Test/hparams.json\n","!cp sp.model models/117MSP_Test/\n","!cp sp.vocab models/117MSP_Test/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSeynQl2A1cB","colab_type":"text"},"source":["## Step 4.3 Encoding the datasets \n","The next cell will encode the raw text-file '`datasets_combined`' by the vocabulary files we just trained for model ''`117MSP_Test`'"]},{"cell_type":"code","metadata":{"id":"1IWWwpnvZQkp","colab_type":"code","colab":{}},"source":["!sh scripts/encode.sh data/datasets_combined.txt 117MSP_Test dataset_books_columns_enc.npz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ag3qN-KWj28T","colab_type":"text"},"source":["Normally, you would want to copy the encoded datasets to your Google Drive for re-using it."]}]}